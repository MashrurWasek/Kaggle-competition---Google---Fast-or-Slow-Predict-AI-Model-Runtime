{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMAmtWljHxV2"
      },
      "outputs": [],
      "source": [
        "# Cloning the TPU Graphs github repo into tpu_graphs_repo\n",
        "\n",
        "!git clone -l -s https://github.com/google-research-datasets/tpu_graphs.git tpu_graphs_repo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install tensorflow_gnn --pre\n",
        "!pip install tensorflow_gnn --pre"
      ],
      "metadata": {
        "id": "yRTWI6zdIV99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install tensorflow_ranking\n",
        "\n",
        "!pip install tensorflow_ranking"
      ],
      "metadata": {
        "id": "rMLK6hBCImtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieving the dataset\n",
        "\n",
        "!python3 tpu_graphs_repo/echo_download_commands.py | bash"
      ],
      "metadata": {
        "id": "_rcaQg9UJn7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install standard modules\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_gnn as tfgnn\n",
        "import tensorflow_ranking as tfr"
      ],
      "metadata": {
        "id": "lIfVn6RwJwpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# copy the utility modules into /content/utility_modules, use sys to allow python to find those modules\n",
        "\n",
        "import sys\n",
        "\n",
        "!mkdir /content/utility_modules\n",
        "\n",
        "sys.path.append(\"/content/utility_modules\")\n",
        "\n",
        "!cp /content/tpu_graphs_repo/tpu_graphs/baselines/layout/data.py /content/utility_modules/tpugraphsv1_layout_data_py.py\n",
        "!cp /content/tpu_graphs_repo/tpu_graphs/baselines/tiles/data.py /content/utility_modules/tpugraphsv1_tile_data_py.py\n",
        "!cp /content/tpu_graphs_repo/tpu_graphs/baselines/tiles/implicit.py /content/utility_modules/tpugraphsv1_implicit_py.py"
      ],
      "metadata": {
        "id": "0ZQg1WHfRpX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use to set toy_data flag to true or false\n",
        "\n",
        "from absl import flags\n",
        "sys.argv=['toy_data=True']\n",
        "flags.FLAGS(sys.argv)"
      ],
      "metadata": {
        "id": "aF3X4yrVNn2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# delete or comment this out in tpugraphsv1_tile_data_py\n",
        "\n",
        "\"\"\"\n",
        "_TOY_DATA = flags.DEFINE_bool(\n",
        "    'toy_data', False,\n",
        "    'If set, then only 5 examples will be used in each of '\n",
        "    '{train, test, validation} partitions.')\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "dKexU1P1XiEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install utility modules.\n",
        "\n",
        "import tpugraphsv1_layout_data_py as layout_data\n",
        "import tpugraphsv1_tile_data_py as tile_data\n",
        "import tpugraphsv1_implicit_py as implicit"
      ],
      "metadata": {
        "id": "u2aCgXS5Jx5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def _mlp(dims, hidden_activation, l2reg=1e-4, use_bias=True):\n",
        "  \"\"\"Helper function for multi-layer perceptron (MLP).\"\"\"\n",
        "  layers = []\n",
        "  for i, dim in enumerate(dims):\n",
        "    if i > 0:\n",
        "      layers.append(tf.keras.layers.Activation(hidden_activation))\n",
        "    layers.append(tf.keras.layers.Dense(\n",
        "        dim, kernel_regularizer=tf.keras.regularizers.l2(l2reg),\n",
        "        use_bias=use_bias))\n",
        "  return tf.keras.Sequential(layers)\n",
        "\n",
        "\n",
        "class _OpEmbedding(tf.keras.Model):\n",
        "  \"\"\"Embeds GraphTensor.node_sets['op']['op'] nodes into feature 'op_e'.\"\"\"\n",
        "\n",
        "  def __init__(self, num_ops: int, embed_d: int, l2reg: float = 1e-4):\n",
        "    super().__init__()\n",
        "    self.embedding_layer = tf.keras.layers.Embedding(\n",
        "        num_ops, embed_d, activity_regularizer=tf.keras.regularizers.l2(l2reg))\n",
        "\n",
        "  def call(\n",
        "      self, graph: tfgnn.GraphTensor,\n",
        "      training: bool = False) -> tfgnn.GraphTensor:\n",
        "    op_features = dict(graph.node_sets['op'].features)\n",
        "    op_features['op_e'] = self.embedding_layer(\n",
        "        tf.cast(graph.node_sets['op']['op'], tf.int32))\n",
        "    return graph.replace_features(node_sets={'op': op_features})\n",
        "\n"
      ],
      "metadata": {
        "id": "Zdg0x_vlJ9Pn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LAYOUT_DATA_ROOT = '~/data/tpugraphs/npz/layout'\n",
        "SOURCE = 'xla'  # Can be \"xla\" or \"nlp\"\n",
        "SEARCH = 'random'  # Can be \"random\" or \"default\"\n",
        "\n",
        "# Batch size information.\n",
        "BATCH_SIZE = 16  # Number of graphs per batch.\n",
        "CONFIGS_PER_GRAPH = 5  # Number of configurations (features and target values) per graph.\n",
        "MAX_KEEP_NODES = 1000  # Useful for dropout.\n",
        "# `MAX_KEEP_NODES` is (or, is not) useful for Segment Dropout, if model uses\n",
        "# edges \"sampled_config\" and \"sampled_feed\" (or, \"config\" and \"feed\")"
      ],
      "metadata": {
        "id": "hNXmzAJrX8Kw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "layout_data_root_dir = os.path.join(\n",
        "      os.path.expanduser(LAYOUT_DATA_ROOT), SOURCE, SEARCH)\n",
        "\n",
        "layout_npz_dataset = layout_data.get_npz_dataset(\n",
        "    layout_data_root_dir,\n",
        "    min_train_configs=CONFIGS_PER_GRAPH,\n",
        "    max_train_configs=500,  # If any graph has more than this configurations, it will be filtered [speeds up loading + training]\n",
        "    cache_dir='cache'\n",
        ")\n",
        "\n",
        "def pair_layout_graph_with_label(graph: tfgnn.GraphTensor):\n",
        "    \"\"\"Extracts label from graph (`tfgnn.GraphTensor`) and returns a pair of `(graph, label)`\"\"\"\n",
        "    # Return runtimes divded over large number: only ranking is required. The\n",
        "    # runtimes are in the 100K range\n",
        "    label = tf.cast(graph.node_sets['g']['runtimes'], tf.float32) / 1e7\n",
        "    return graph, label\n",
        "\n",
        "layout_train_ds = (\n",
        "      layout_npz_dataset.train.get_graph_tensors_dataset(\n",
        "          CONFIGS_PER_GRAPH, max_nodes=MAX_KEEP_NODES)\n",
        "      .shuffle(100, reshuffle_each_iteration=True)\n",
        "      .batch(BATCH_SIZE, drop_remainder=False)\n",
        "      .map(tfgnn.GraphTensor.merge_batch_to_components)\n",
        "      .map(pair_layout_graph_with_label))\n",
        "\n",
        "layout_valid_ds = (\n",
        "      layout_npz_dataset.validation.get_graph_tensors_dataset(\n",
        "          CONFIGS_PER_GRAPH)\n",
        "      .batch(BATCH_SIZE, drop_remainder=False)\n",
        "      .map(tfgnn.GraphTensor.merge_batch_to_components)\n",
        "      .map(pair_layout_graph_with_label))"
      ],
      "metadata": {
        "id": "qTkcTwB1YTWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph_batch, config_runtimes = next(iter(layout_train_ds.take(1)))\n",
        "\n",
        "print('graph_batch = ')\n",
        "print(graph_batch)\n",
        "print('\\n\\n')\n",
        "print('config_runtimes=')\n",
        "print(config_runtimes)"
      ],
      "metadata": {
        "id": "p05Do-MaZNSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The `graph_batch` contains node-sets and edge-sets.\n",
        "# There are no context features for layout collection\n",
        "print('graph_batch.context =', graph_batch.context)\n",
        "# Note: graph_batch.context.sizes must be equal to BATCH_SIZE.\n",
        "# Lets print-out all features for all nodesets.\n",
        "\n",
        "for node_set_name in sorted(graph_batch.node_sets.keys()):\n",
        "    print(f'\\n\\n #####  NODE SET \"{node_set_name}\" #########')\n",
        "    print('** Has sizes: ', graph_batch.node_sets[node_set_name].sizes)\n",
        "    for feature_name in graph_batch.node_sets[node_set_name].features.keys():\n",
        "        print(f'\\n Feature \"{feature_name}\" has values')\n",
        "        print(graph_batch.node_sets[node_set_name][feature_name])\n"
      ],
      "metadata": {
        "id": "9WUGA0JBZNx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\n config edge set: ', graph_batch.edge_sets['config'])\n",
        "print('\\n config source nodes: ', graph_batch.edge_sets['config'].adjacency.source)\n",
        "print('\\n config target nodes: ', graph_batch.edge_sets['config'].adjacency.target)\n",
        "print('\\n g_op edge set: ', graph_batch.edge_sets['g_op'])\n",
        "print('\\n g_config edge set: ', graph_batch.edge_sets['g_config'])"
      ],
      "metadata": {
        "id": "NjRl3gC-fNI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_ops = layout_npz_dataset.num_ops\n",
        "print('number of ops in the dataset=', num_ops)\n",
        "\n",
        "embedding_layer = _OpEmbedding(num_ops, 16)  # 16-dimensional embedding, for demonstration.\n",
        "graph_batch_embedded_ops = embedding_layer(graph_batch)\n",
        "\n",
        "print('\\n\\n Before embedding, node-set \"op\"=\\n', graph_batch.node_sets['op'])\n",
        "print('\\n\\n After embedding, node-set \"op\"=\\n', graph_batch_embedded_ops.node_sets['op'])"
      ],
      "metadata": {
        "id": "3CtGgR5nfVaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "op_e = graph_batch_embedded_ops.node_sets['op']['op_e']\n",
        "config_features = graph_batch_embedded_ops.node_sets['nconfig']['feats']\n",
        "\n",
        "print('op_e.shape ==', op_e.shape)\n",
        "print('config_features.shape ==', config_features.shape)"
      ],
      "metadata": {
        "id": "yxg2JPI5gBC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config_adj = implicit.AdjacencyMultiplier(graph_batch_embedded_ops, 'config')\n",
        "print('config_adj.shape =', config_adj.shape)\n",
        "resized_config_features = config_adj @ config_features\n",
        "print('resized_config_features.shape =', resized_config_features.shape)"
      ],
      "metadata": {
        "id": "auCsfO95gDVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "broadcasted_op_e = tf.stack([op_e] * CONFIGS_PER_GRAPH, axis=1)\n",
        "\n",
        "combined_features = tf.concat([broadcasted_op_e, resized_config_features], axis=-1)\n",
        "\n",
        "print('combined_features.shape = ', combined_features.shape)"
      ],
      "metadata": {
        "id": "KtR4hjgTgF6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adj_op_op = implicit.AdjacencyMultiplier(graph_batch_embedded_ops, 'feed')  # op->op\n",
        "adj_config = implicit.AdjacencyMultiplier(graph_batch_embedded_ops, 'config')  # nconfig->op\n",
        "\n",
        "adj_op_op_hat = (adj_op_op + adj_op_op.transpose()).add_eye()\n",
        "adj_op_op_hat = adj_op_op_hat.normalize_symmetric()"
      ],
      "metadata": {
        "id": "p5lx6rbwgH93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A_times_X = adj_op_op_hat @ combined_features\n",
        "print('A_times_x.shape =', A_times_X.shape)"
      ],
      "metadata": {
        "id": "TCCz2WWNgKiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResModel(tf.keras.Model):\n",
        "    \"\"\"GNN with residual connections.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, num_configs: int, num_ops: int, op_embed_dim: int = 32,\n",
        "        num_gnns: int = 2, mlp_layers: int = 2,\n",
        "        hidden_activation: str = 'leaky_relu',\n",
        "        hidden_dim: int = 32, reduction: str = 'sum'):\n",
        "        super().__init__()\n",
        "        self._num_configs = num_configs\n",
        "        self._num_ops = num_ops\n",
        "        self._op_embedding = _OpEmbedding(num_ops, op_embed_dim)\n",
        "        self._prenet = _mlp([hidden_dim] * mlp_layers, hidden_activation)\n",
        "        self._gc_layers = []\n",
        "        for _ in range(num_gnns):\n",
        "            self._gc_layers.append(_mlp([hidden_dim] * mlp_layers, hidden_activation))\n",
        "        self._postnet = _mlp([hidden_dim, 1], hidden_activation, use_bias=False)\n",
        "\n",
        "    def call(self, graph: tfgnn.GraphTensor, training: bool = False):\n",
        "        del training\n",
        "        return self.forward(graph, self._num_configs)\n",
        "\n",
        "    def _node_level_forward(\n",
        "        self, node_features: tf.Tensor,\n",
        "        config_features: tf.Tensor,\n",
        "        graph: tfgnn.GraphTensor, num_configs: int,\n",
        "        edgeset_prefix='') -> tf.Tensor:\n",
        "        adj_op_op = implicit.AdjacencyMultiplier(\n",
        "            graph, edgeset_prefix+'feed')  # op->op\n",
        "        adj_config = implicit.AdjacencyMultiplier(\n",
        "            graph, edgeset_prefix+'config')  # nconfig->op\n",
        "\n",
        "        adj_op_op_hat = (adj_op_op + adj_op_op.transpose()).add_eye()\n",
        "        adj_op_op_hat = adj_op_op_hat.normalize_symmetric()\n",
        "\n",
        "        x = node_features\n",
        "\n",
        "        x = tf.stack([x] * num_configs, axis=1)\n",
        "        config_features = 100 * (adj_config @ config_features)\n",
        "        x = tf.concat([config_features, x], axis=-1)\n",
        "        x = self._prenet(x)\n",
        "        x = tf.nn.leaky_relu(x)\n",
        "\n",
        "        for layer in self._gc_layers:\n",
        "            y = x\n",
        "            y = tf.concat([config_features, y], axis=-1)\n",
        "            y = tf.nn.leaky_relu(layer(adj_op_op_hat @ y))\n",
        "            x += y\n",
        "        return x\n",
        "\n",
        "    def forward(\n",
        "        self, graph: tfgnn.GraphTensor, num_configs: int,\n",
        "        backprop=True) -> tf.Tensor:\n",
        "        graph = self._op_embedding(graph)\n",
        "\n",
        "        config_features = graph.node_sets['nconfig']['feats']\n",
        "        node_features = tf.concat([\n",
        "            graph.node_sets['op']['feats'],\n",
        "            graph.node_sets['op']['op_e']\n",
        "        ], axis=-1)\n",
        "\n",
        "        x_full = self._node_level_forward(\n",
        "            node_features=tf.stop_gradient(node_features),\n",
        "            config_features=tf.stop_gradient(config_features),\n",
        "            graph=graph, num_configs=num_configs)\n",
        "\n",
        "        if backprop:\n",
        "            x_backprop = self._node_level_forward(\n",
        "                node_features=node_features,\n",
        "                config_features=config_features,\n",
        "                graph=graph, num_configs=num_configs, edgeset_prefix='sampled_')\n",
        "\n",
        "            is_selected = graph.node_sets['op']['selected']\n",
        "            # Need to expand twice as `is_selected` is a vector (num_nodes) but\n",
        "            # x_{backprop, full} are 3D tensors (num_nodes, num_configs, num_feats).\n",
        "            is_selected = tf.expand_dims(is_selected, axis=-1)\n",
        "            is_selected = tf.expand_dims(is_selected, axis=-1)\n",
        "            x = tf.where(is_selected, x_backprop, x_full)\n",
        "        else:\n",
        "            x = x_full\n",
        "\n",
        "        adj_config = implicit.AdjacencyMultiplier(graph, 'config')\n",
        "\n",
        "        # Features for configurable nodes.\n",
        "        config_feats = (adj_config.transpose() @ x)\n",
        "\n",
        "        # Global pooling\n",
        "        adj_pool_op_sum = implicit.AdjacencyMultiplier(graph, 'g_op').transpose()\n",
        "        adj_pool_op_mean = adj_pool_op_sum.normalize_right()\n",
        "        adj_pool_config_sum = implicit.AdjacencyMultiplier(\n",
        "            graph, 'g_config').transpose()\n",
        "        x = self._postnet(tf.concat([\n",
        "            # (A D^-1) @ Features\n",
        "            adj_pool_op_mean @ x,\n",
        "            # l2_normalize( A @ Features )\n",
        "            tf.nn.l2_normalize(adj_pool_op_sum @ x, axis=-1),\n",
        "            # l2_normalize( A @ Features )\n",
        "            tf.nn.l2_normalize(adj_pool_config_sum @ config_feats, axis=-1),\n",
        "        ], axis=-1))\n",
        "\n",
        "        x = tf.squeeze(x, -1)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "nUiPrRSOgMnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ResModel(CONFIGS_PER_GRAPH, layout_npz_dataset.num_ops)\n",
        "\n",
        "loss = tfr.keras.losses.ListMLELoss()  # (temperature=10)\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=1e-3, clipnorm=0.5)\n",
        "\n",
        "model.compile(loss=loss, optimizer=opt, metrics=[\n",
        "    tfr.keras.metrics.OPAMetric(name='opa_metric'),\n",
        "])\n"
      ],
      "metadata": {
        "id": "qFrWUHmNgTmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early_stop = 5  # If validation OPA did not increase in this many epochs, terminate training.\n",
        "best_params = None  # Stores parameters corresponding to best validation OPA, to restore to them after training.\n",
        "best_val_opa = -1  # Tracks best validation OPA\n",
        "best_val_at_epoch = -1  # At which epoch.\n",
        "epochs = 1  # Total number of training epochs.\n",
        "\n",
        "for i in range(epochs):\n",
        "    history = model.fit(\n",
        "        layout_train_ds, epochs=1, verbose=1, validation_data=layout_valid_ds,\n",
        "        validation_freq=1)\n",
        "\n",
        "    train_loss = history.history['loss'][-1]\n",
        "    train_opa = history.history['opa_metric'][-1]\n",
        "    val_loss = history.history['val_loss'][-1]\n",
        "    val_opa = history.history['val_opa_metric'][-1]\n",
        "    if val_opa > best_val_opa:\n",
        "        best_val_opa = val_opa\n",
        "        best_val_at_epoch = i\n",
        "        best_params = {v.ref: v + 0 for v in model.trainable_variables}\n",
        "        print(' * [@%i] Validation (NEW BEST): %s' % (i, str(val_opa)))\n",
        "    elif early_stop > 0 and i - best_val_at_epoch >= early_stop:\n",
        "      print('[@%i] Best accuracy was attained at epoch %i. Stopping.' % (i, best_val_at_epoch))\n",
        "      break\n",
        "\n",
        "# Restore best parameters.\n",
        "print('Restoring parameters corresponding to the best validation OPA.')\n",
        "assert best_params is not None\n",
        "for v in model.trainable_variables:\n",
        "    v.assign(best_params[v.ref])"
      ],
      "metadata": {
        "id": "TlzTeFAsgXyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iIzrc417gaZL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}